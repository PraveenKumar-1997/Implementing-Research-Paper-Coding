# -*- coding: utf-8 -*-
"""Sparse K-Means with dimensionality reduction for large data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15usbn4a-j3kfDQfOIYgeDEyYaEKBxfA-
"""

from sklearn.datasets import load_iris
data = load_iris()
X = data['data']
import numpy as np
import pandas as pd
import sys

def distance(p1, p2):
    return np.sum((p1 - p2)**2)
    
def initialize(data, k):
    '''
    initialized the centroids for K-means++
    inputs:
        data - numpy array of data points having shape (200, 2)
        k - number of clusters
    '''
    ## initialize the centroids list and add
    ## a randomly selected data point to the list
    centroids = []
    centroids.append(data[np.random.randint(
            data.shape[0]), :])

  
    ## compute remaining k - 1 centroids
    for c_id in range(k - 1):
         
        ## initialize a list to store distances of data
        ## points from nearest centroid
        dist = []
        for i in range(data.shape[0]):
            point = data[i, :]
            d = sys.maxsize
             
            ## compute distance of 'point' from each of the previously
            ## selected centroid and store the minimum distance
            for j in range(len(centroids)):
                temp_dist = distance(point, centroids[j])
                d = min(d, temp_dist)
            dist.append(d)
             
        ## select data point with maximum distance as our next centroid
        dist = np.array(dist)
        next_centroid = data[np.argmax(dist), :]
        centroids.append(next_centroid)
        dist = []

    return np.vstack(centroids)

def get_centroid_index(x):
    
  def distance_for_fixed_x(centroid):

    return np.sum((centroid - x)**2)

  distance_vector = np.apply_along_axis(distance_for_fixed_x,axis = 1,arr = centroids)

  return np.argmin(distance_vector)

def WCSS(X,centroids,centroid_indices):
  distance = 0
  for cluster_id in range(no_of_centroids):

    centroid = centroids[cluster_id,:]

    def distance_for_fixed_centroid(x):
      return np.sum((x - centroid)**2)

    point_indices, = np.where(centroid_indices == cluster_id)
    points = np.take(X, point_indices, axis=0)
    # print(points.shape)
    # print(centroid.shape)
    distance += np.sum(np.apply_along_axis(distance_for_fixed_centroid,axis = 1,arr = points))
  
  return distance
# call the initialize function to get the centroids

iterations = 100
no_of_centroids = 4
tol = 0.001
s = 2
centroids = initialize(X, k = no_of_centroids)
centroid_indices = np.apply_along_axis(get_centroid_index,axis = 1,arr = X)

number_of_features = X.shape[1]
loss = WCSS(X,centroids,centroid_indices)

for i in range(iterations):
  new_centroids = []
  points_in_centroid = []
  old_loss = loss.copy()

  for cluster_id in range(no_of_centroids):
    #get points from centroid indices
    point_indices, = np.where(centroid_indices == cluster_id)
    points = np.take(X, point_indices, axis=0)
    points_in_centroid.append(points.shape[0])
    #get centroid location and append
    new_centroids.append(np.mean(points,axis = 0))
  
  #get new centroids in numpy array format
  new_centroids = np.vstack(new_centroids)
  points_in_centroid = np.array(points_in_centroid)


  #calculate feature importance
  dl = np.sum(new_centroids**2,axis = 0)/points_in_centroid
  print("dl")
  print(dl)
  dl_argsort = np.argsort(-1*dl)
  print("dl_argsort")
  print(dl_argsort)

  new_centroids_post_ranking = new_centroids.copy()
  dl_id = 0

  for dl_loc in dl_argsort:
    #dl_loc location of the dl_id largest value in dl array
    if dl_id < s:
      continue

    new_centroids_post_ranking[:,dl_loc] = np.zeros((new_centroids_post_ranking.shape[0],))

    dl_id+=1

  print("new_centroids")
  print(new_centroids_post_ranking)

  centroids = new_centroids_post_ranking
  centroid_indices = np.apply_along_axis(get_centroid_index,axis = 1,arr = X)
  print("centroid_indices")
  print(loss)
  loss = WCSS(X,centroids,centroid_indices)
  print(loss)
  if np.abs(loss - old_loss)<= tol:
    print('Loss Stabilised at %d iterations'%(i))
    break

def plot_data(X):
    plt.scatter(
    X[:, 0], X[:, 1],
    c='white', marker='o',
    edgecolor='black', s=50
    )
    plt.show()

def plot_clusters(X,centroids,cluster):
    sns.scatterplot(X[:,0], X[:, 1], hue=cluster)
    sns.scatterplot(centroids[:,0], centroids[:, 1], s=100, color='y')
    plt.xlabel('feature1')
    plt.ylabel('feature2')
    plt.show()

def plot_loss(loss_list):
    epochs = [i for i in range(len(loss_list))]
    plt.plot(epochs,loss_list)
    plt.xlabel('epochs')
    plt.ylabel('loss')
    plt.show()

iterations = 100
no_of_centroids = 4
tol = 0.001
s = 1

centroids = initialize(X, k = no_of_centroids)
centroid_indices = np.apply_along_axis(get_centroid_index,axis = 1,arr = X)

number_of_features = X.shape[1]
loss = WCSS(X,centroids,centroid_indices)

for i in range(iterations):
  new_centroids = []
  points_in_centroid = []
  old_loss = loss.copy()

  for cluster_id in range(no_of_centroids):
    #get points from centroid indices
    point_indices, = np.where(centroid_indices == cluster_id)
    points = np.take(X, point_indices, axis=0)
    points_in_centroid.append(points.shape[0])
    #get centroid location and append
    new_centroids.append(np.mean(points,axis = 0))
  
  #get new centroids in numpy array format
  new_centroids = np.vstack(new_centroids)
  points_in_centroid = np.array(points_in_centroid)
  # print(new_centroids)
  # print(points_in_centroid)

  #calculate feature importance
  dl = np.sum(new_centroids**2*points_in_centroid[:,np.newaxis],axis = 0)
  # print("dl")
  # print(dl)
  dl_argsort = np.argsort(-1*dl)
  # print("dl_argsort")
  # print(dl_argsort)

  new_centroids_post_ranking = new_centroids.copy()
  dl_id = 0

  for dl_loc in dl_argsort:
    #dl_loc location of the dl_id largest value in dl array
    if dl_id < s:
      dl_id+=1
      continue

    new_centroids_post_ranking[:,dl_loc] = np.zeros((new_centroids_post_ranking.shape[0],))
    dl_id+=1
    

  print("new_centroids")
  print(new_centroids_post_ranking)
  centroids_old = centroids.copy()
  centroids = new_centroids_post_ranking
  centroid_indices = np.apply_along_axis(get_centroid_index,axis = 1,arr = X)
  centroids = centroids_old.copy()
  # print("centroid_indices")
  # print(loss)
  loss = WCSS(X,centroids,centroid_indices)
  # print(loss)
  if np.abs(loss - old_loss)<= tol:
    print('Loss Stabilised at %d iterations'%(i))
    break

import matplotlib.pyplot as plt
plt.scatter(X[:,0],X[:,1],c = centroid_indices)
plt.show()

